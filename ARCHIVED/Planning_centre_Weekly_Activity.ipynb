{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoCA-ui736os"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vzI4ZpU-P_2N"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "\n",
    "try:\n",
    "  import pandas as pd\n",
    "except ModuleNotFoundError:\n",
    "  !pip install pandas\n",
    "  import pandas as pd\n",
    "\n",
    "from datetime import datetime, timezone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHC8lf974z81"
   },
   "source": [
    "## Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK3R4kVaOrQj"
   },
   "outputs": [],
   "source": [
    "# Planning centre URLs\n",
    "BASE_URL_SERVICES = 'https://api.planningcenteronline.com/services/v2/'\n",
    "PEOPLE_BASE_URL = 'https://api.planningcenteronline.com/people/v2/people/'\n",
    "CAMPUS_BASE_URL = 'https://api.planningcenteronline.com/people/v2/campuses'\n",
    "BASE_URL_WORKFLOW = 'https://api.planningcenteronline.com/people/v2/workflows/'\n",
    "\n",
    "# Planning centre API\n",
    "API_APP_ID = \" \"\n",
    "API_SECRET = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvX_yfex6O6a"
   },
   "source": [
    "## OPTIONAL: Retrieve all the workflow IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73o6WswGE2Zq",
    "outputId": "9ca8b704-ff24-4bd1-d21c-9cbbffa23b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Workflows: 35\n",
      "Workflow IDs: ['548715', '544795', '544778', '564585', '544755', '544593', '550431', '550408', '544671', '544864', '544777', '544842', '550389', '541922', '550371', '544885', '544809', '564649', '548180', '544664', '550421', '544725', '550386', '544846', '555089', '564646', '548148', '544649', '550418', '544718', '550368', '544881', '544844', '550397', '561167']\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch all workflow IDs\n",
    "def get_all_workflow_ids():\n",
    "    workflow_ids = []\n",
    "    url = BASE_URL_WORKFLOW\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url, auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            workflows_data = response.json()\n",
    "\n",
    "            # Extract workflow IDs from the current page\n",
    "            for workflow in workflows_data['data']:\n",
    "                workflow_ids.append(workflow['id'])\n",
    "\n",
    "            # Check if there is a next page\n",
    "            url = workflows_data['links'].get('next')\n",
    "        else:\n",
    "            print(f\"Error fetching workflows: {response.status_code}, {response.json()}\")\n",
    "            break\n",
    "\n",
    "    return workflow_ids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    workflow_ids = get_all_workflow_ids()\n",
    "    print(f\"Total Workflows: {len(workflow_ids)}\")\n",
    "    print(\"Workflow IDs:\", workflow_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPepJe-S6YPT"
   },
   "source": [
    "## Retrieve information on all members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4SX4pv-DOeW",
    "outputId": "81776c04-1212-4d71-929c-0f28b0240743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'people_data.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "def fetch_all_people(per_page=100):\n",
    "    url = f'{PEOPLE_BASE_URL}'\n",
    "    all_results = []\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        # Fetch a batch of data\n",
    "        response = requests.get(f'{url}?per_page={per_page}&offset={offset}', auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data['data']:  # No more data to fetch\n",
    "                break\n",
    "\n",
    "            all_results.extend(data['data'])  # Append results to the list\n",
    "\n",
    "            # Update offset to fetch the next batch\n",
    "            offset += per_page\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}, {response.json()}\")\n",
    "            break\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def save_people_data_to_csv(people_data, csv_filename='people_data.csv'):\n",
    "    \"\"\"\n",
    "    Saves a list of people data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        people_data (list): A list of dictionaries containing people data.\n",
    "        csv_filename (str): The name of the CSV file to save the data to.\n",
    "    \"\"\"\n",
    "    # Define the columns for the CSV\n",
    "    csv_columns = [\n",
    "        'id', 'first_name', 'last_name', 'name', 'birthdate', 'gender', 'membership',\n",
    "        'status', 'created_at', 'updated_at', 'avatar', 'accounting_administrator',\n",
    "        'can_create_forms', 'can_email_lists', 'child', 'passed_background_check',\n",
    "        'people_permissions', 'site_administrator', 'primary_campus_id'\n",
    "    ]\n",
    "\n",
    "    # Open CSV file for writing\n",
    "    try:\n",
    "        with open(csv_filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write each person entry to the CSV file\n",
    "            for person in people_data:\n",
    "                attributes = person['attributes']\n",
    "                relationships = person['relationships']\n",
    "\n",
    "                row = {\n",
    "                    'id': person['id'],\n",
    "                    'first_name': attributes.get('first_name'),\n",
    "                    'last_name': attributes.get('last_name'),\n",
    "                    'name': attributes.get('name'),\n",
    "                    'birthdate': attributes.get('birthdate'),\n",
    "                    'gender': attributes.get('gender'),\n",
    "                    'membership': attributes.get('membership'),\n",
    "                    'status': attributes.get('status'),\n",
    "                    'created_at': attributes.get('created_at'),\n",
    "                    'updated_at': attributes.get('updated_at'),\n",
    "                    'avatar': attributes.get('avatar'),\n",
    "                    'accounting_administrator': attributes.get('accounting_administrator'),\n",
    "                    'can_create_forms': attributes.get('can_create_forms'),\n",
    "                    'can_email_lists': attributes.get('can_email_lists'),\n",
    "                    'child': attributes.get('child'),\n",
    "                    'passed_background_check': attributes.get('passed_background_check'),\n",
    "                    'people_permissions': attributes.get('people_permissions'),\n",
    "                    'site_administrator': attributes.get('site_administrator'),\n",
    "                    #'primary_campus_id':relationships.get('primary_campus', {}).get('data', {}).get('id')\n",
    "                }\n",
    "\n",
    "                writer.writerow(row)\n",
    "\n",
    "        print(f\"CSV file '{csv_filename}' created successfully.\")\n",
    "\n",
    "    except IOError:\n",
    "        print(\"I/O error while writing the CSV file.\")\n",
    "\n",
    "people = fetch_all_people()\n",
    "save_people_data_to_csv(people_data=people)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "people_df = pd.read_csv('people_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLPOhupq7A2z"
   },
   "source": [
    "## Get Connect Group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtLTlzBGIhZW"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_person_name(person_id):\n",
    "    \"\"\"\n",
    "    Function to get a person's name using their person_id from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        person_id (str): The ID of the person.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the person, or \"Unknown Person\" if not found.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to find the row matching the person_id\n",
    "    person = people_df[people_df['id'].astype(str) == person_id]\n",
    "\n",
    "    if not person.empty:\n",
    "        return person.iloc[0]['name']\n",
    "    else:\n",
    "        return \"Unknown Person\"\n",
    "\n",
    "def get_person_email(person_id):\n",
    "    \"\"\"\n",
    "    Function to get a person's email using their person_id from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        person_id (str): The ID of the person.\n",
    "\n",
    "    Returns:\n",
    "        str: The email of the person, or \"No Email Found\" if not found.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to find the row matching the person_id\n",
    "    person = people_df[people_df['id'].astype(str) == person_id]\n",
    "\n",
    "    if not person.empty:\n",
    "        # Assuming 'email' column contains the email, adjust if different\n",
    "        return person.iloc[0].get('email', \"No Email Found\")\n",
    "    else:\n",
    "        return \"No Email Found\"\n",
    "\n",
    "def get_person_email(person_id):\n",
    "    url = f'{PEOPLE_BASE_URL}{person_id}/emails'\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        emails_data = response.json()\n",
    "        if emails_data['data']:\n",
    "            primary_email = next((email['attributes']['address'] for email in emails_data['data'] if email['attributes']['primary']), emails_data['data'][0]['attributes']['address'])\n",
    "            return primary_email\n",
    "        else:\n",
    "            return \"No Email Found\"\n",
    "    else:\n",
    "        print(f\"Error fetching email for person {person_id}: {response.status_code}, {response.json()}\")\n",
    "        return \"Unknown Email\"\n",
    "\n",
    "def get_person_email(person_id, retries=5, batch_size=100):\n",
    "    \"\"\"\n",
    "    Function to get a person's email using their person_id, with pagination using offset.\n",
    "\n",
    "    Args:\n",
    "        person_id (str): The ID of the person.\n",
    "        retries (int): The number of retries in case of rate-limiting.\n",
    "        batch_size (int): The number of records per batch (default is 100).\n",
    "\n",
    "    Returns:\n",
    "        str: The primary email of the person, or \"No Email Found\" if not found.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    all_emails = []\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        while True:\n",
    "            url = f'{PEOPLE_BASE_URL}{person_id}/emails?offset={offset}&per_page={batch_size}'\n",
    "            response = requests.get(url, auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                emails_data = response.json()\n",
    "\n",
    "                # Append all emails fetched in this batch\n",
    "                all_emails.extend(emails_data['data'])\n",
    "\n",
    "                # If there are no more emails to fetch, break the loop\n",
    "                if not emails_data['data'] or 'next' not in emails_data['links']:\n",
    "                    break\n",
    "\n",
    "                # Move to the next page by updating the offset\n",
    "                offset += batch_size\n",
    "\n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                retry_after = response.headers.get('Retry-After')\n",
    "                if retry_after:\n",
    "                    wait_time = int(retry_after)\n",
    "                else:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff if Retry-After is not provided\n",
    "                print(f\"Rate limit exceeded. Waiting {wait_time} seconds before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "            else:\n",
    "                print(f\"Error fetching email for person {person_id}: {response.status_code}, {response.json()}\")\n",
    "                return \"No Email Found\"\n",
    "\n",
    "    # After fetching all pages, find the primary email or return the first email found\n",
    "    if all_emails:\n",
    "        primary_email = next((email['attributes']['address'] for email in all_emails if email['attributes']['primary']), all_emails[0]['attributes']['address'])\n",
    "        return primary_email\n",
    "    else:\n",
    "        return \"No Email Found\"\n",
    "# Function to get the list of workflow steps\n",
    "def get_workflow_steps(workflow_id):\n",
    "    url = f'{BASE_URL_WORKFLOW}{workflow_id}/steps'\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error fetching steps for workflow {workflow_id}: {response.status_code}, {response.json()}\")\n",
    "        return None\n",
    "\n",
    "# Function to get all people (cards) in the workflow\n",
    "def get_workflow_people(workflow_id, retries=5, batch_size=100):\n",
    "    \"\"\"\n",
    "    Fetches all people (workflow cards) from the specified workflow using pagination with offset.\n",
    "\n",
    "    Args:\n",
    "        workflow_id (str): The ID of the workflow.\n",
    "        retries (int): The number of retries in case of rate-limiting or network issues.\n",
    "        batch_size (int): The number of people to fetch per batch (default is 100).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of all people (cards) in the workflow, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    url = f'{BASE_URL_WORKFLOW}/{workflow_id}/cards'\n",
    "    all_people = []\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        while True:\n",
    "            # Fetch a batch of people using pagination with offset\n",
    "            response = requests.get(f'{url}?offset={offset}&per_page={batch_size}', auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                people_data = response.json()\n",
    "                all_people.extend(people_data['data'])  # Append the people data to the result list\n",
    "\n",
    "                # If no more data, break out of the loop\n",
    "                if not people_data['data'] or 'next' not in people_data['links']:\n",
    "                    break\n",
    "\n",
    "                # Move to the next page by updating the offset\n",
    "                offset += batch_size\n",
    "\n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                retry_after = response.headers.get('Retry-After')\n",
    "                if retry_after:\n",
    "                    wait_time = int(retry_after)\n",
    "                else:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff if Retry-After is not provided\n",
    "                print(f\"Rate limit exceeded. Waiting {wait_time} seconds before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "            else:\n",
    "                print(f\"Error fetching people for workflow {workflow_id}: {response.status_code}, {response.json()}\")\n",
    "                return None\n",
    "\n",
    "        return all_people  # Return the list of all people after fetching all pages\n",
    "\n",
    "    print(f\"Failed to fetch workflow people after {retries} retries.\")\n",
    "    return None\n",
    "\n",
    "# Function to calculate how long someone has been in a step\n",
    "def calculate_days_in_step(moved_to_step_at):\n",
    "    moved_to_step_at_datetime = datetime.strptime(moved_to_step_at, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=timezone.utc)\n",
    "    now = datetime.now(timezone.utc)\n",
    "    days_in_step = (now - moved_to_step_at_datetime).days\n",
    "    return days_in_step\n",
    "\n",
    "# Function to get the campus data and map campus IDs to names\n",
    "def get_campus_mapping():\n",
    "    url = CAMPUS_BASE_URL\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        campus_data = response.json()\n",
    "        campus_mapping = {campus['id']: campus['attributes']['name'] for campus in campus_data['data']}\n",
    "        return campus_mapping\n",
    "    else:\n",
    "        print(f\"Error fetching campuses: {response.status_code}, {response.json()}\")\n",
    "        return {}\n",
    "\n",
    "def get_card_id_for_person_in_workflow(workflow_id, person_id, retries=5, per_page=100):\n",
    "    \"\"\"\n",
    "    Function to get the card ID for a person in a workflow with pagination and retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        workflow_id (str): The ID of the workflow.\n",
    "        person_id (str): The ID of the person.\n",
    "        retries (int): Number of retry attempts in case of rate-limiting.\n",
    "        per_page (int): Number of results to fetch per page (default is 100).\n",
    "\n",
    "    Returns:\n",
    "        str: The card ID for the person, or None if not found.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    url = f'{BASE_URL_WORKFLOW}/{workflow_id}/cards'\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        while True:\n",
    "            # Fetch a batch of workflow cards using pagination with offset\n",
    "            response = requests.get(f'{url}?per_page={per_page}&offset={offset}', auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                cards_data = response.json()\n",
    "\n",
    "                # Search for the card with the matching person_id\n",
    "                for card in cards_data['data']:\n",
    "                    card_person_id = card['relationships']['person']['data']['id']\n",
    "                    if card_person_id == person_id:\n",
    "                        return card['id']  # This is the card_id\n",
    "\n",
    "                # If there are no more cards, break out of the loop\n",
    "                if not cards_data['data'] or 'next' not in cards_data['links']:\n",
    "                    break\n",
    "\n",
    "                # Move to the next page\n",
    "                offset += per_page\n",
    "\n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                # Check if a \"Retry-After\" header is provided\n",
    "                retry_after = response.headers.get('Retry-After')\n",
    "                if retry_after:\n",
    "                    wait_time = int(retry_after)\n",
    "                else:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff if Retry-After is not provided\n",
    "                print(f\"Rate limit exceeded. Waiting {wait_time} seconds before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "            else:\n",
    "                print(f\"Error fetching workflow cards: {response.status_code}, {response.json()}\")\n",
    "                return None\n",
    "\n",
    "        print(f\"No card found for person ID: {person_id}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Failed to fetch workflow cards after {retries} retries.\")\n",
    "    return None\n",
    "\n",
    "# Function to get activities for a workflow card and filter by \"completed the step\"\n",
    "def get_workflow_step_history_from_activities(workflow_id, card_id):\n",
    "    url = f'{BASE_URL_WORKFLOW}/{workflow_id}/cards/{card_id}/activities' #Changed WORKFLOW\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        activities_data = response.json()\n",
    "        activities = activities_data['data']\n",
    "        step_history = [activity for activity in activities]\n",
    "\n",
    "        return step_history\n",
    "    else:\n",
    "        print(f\"Error fetching activities: {response.status_code}, {response.json()}\")\n",
    "        return None\n",
    "\n",
    "# Function to retrieve step history for CSV export\n",
    "def get_step_history_for_csv(workflow_id, person_id):\n",
    "    card_id = get_card_id_for_person_in_workflow(workflow_id, person_id)\n",
    "    if card_id:\n",
    "        step_history = get_workflow_step_history_from_activities(workflow_id, card_id)\n",
    "        history_str = \"\"\n",
    "        if step_history:\n",
    "            for activity in step_history:\n",
    "                comment = activity['attributes']['comment']\n",
    "                created_at = activity['attributes']['created_at']\n",
    "                history_str += f\"{comment} on {created_at}; \"\n",
    "        return history_str.strip(\"; \")\n",
    "    return \"No step history\"\n",
    "\n",
    "# Main function to list people by workflow step, with additional fields and export to CSV\n",
    "def list_people_by_workflow_step_to_csv(workflow_id, csv_filename='workflow_data.csv'):\n",
    "\n",
    "    workflow_steps = get_workflow_steps(workflow_id)\n",
    "    workflow_info_url = f'{BASE_URL_WORKFLOW}{workflow_id}'\n",
    "\n",
    "    # Fetch campus mapping\n",
    "    campus_mapping = get_campus_mapping()\n",
    "\n",
    "    # Fetch workflow info to get the name and campus ID\n",
    "    workflow_info_response = requests.get(workflow_info_url, auth=HTTPBasicAuth(API_APP_ID, API_SECRET))\n",
    "\n",
    "    if workflow_info_response.status_code == 200:\n",
    "        workflow_info = workflow_info_response.json()\n",
    "        workflow_name = workflow_info['data']['attributes']['name']\n",
    "\n",
    "        try:\n",
    "          campus_id = workflow_info['data']['relationships'].get('campus', {}).get('data', {}).get('id')\n",
    "          campus_name = campus_mapping.get(campus_id, 'Unknown Campus')\n",
    "        except:\n",
    "          campus_name = 'Unknown Campus'\n",
    "\n",
    "    else:\n",
    "        print(f\"Error fetching workflow info: {workflow_info_response.status_code}\")\n",
    "        return\n",
    "\n",
    "    if workflow_steps:\n",
    "        steps_dict = {step['id']: step['attributes']['name'] for step in workflow_steps['data']}\n",
    "        people_in_workflow = get_workflow_people(workflow_id)\n",
    "\n",
    "        if people_in_workflow:\n",
    "            data_rows = []\n",
    "\n",
    "            for person_card in people_in_workflow:\n",
    "                person_id = person_card['relationships']['person']['data']['id']\n",
    "                assignee_id = person_card['relationships'].get('assignee', {}).get('data', {}).get('id')\n",
    "                try:\n",
    "                  workflow_step = steps_dict[current_step_id]\n",
    "                except:\n",
    "                  workflow_step = None\n",
    "\n",
    "                try:\n",
    "                  current_step_id = person_card['relationships']['current_step']['data']['id']\n",
    "                except:\n",
    "                  current_step_id = None\n",
    "                person_name = get_person_name(person_id)\n",
    "\n",
    "                # Get assignee name and email\n",
    "                assignee_name = get_person_name(assignee_id) if assignee_id else \"Unknown Assignee\"\n",
    "                assignee_email = get_person_email(assignee_id) if assignee_id else \"Unknown Email\"\n",
    "\n",
    "                # Get the date they were moved to the current step\n",
    "                moved_to_step_at = person_card['attributes'].get('moved_to_step_at', None)\n",
    "                removed_at = person_card['attributes'].get('removed_at', None)\n",
    "                completed_at = person_card['attributes'].get('completed_at', None)\n",
    "\n",
    "                # Determine removal status and calculate days in step\n",
    "                if moved_to_step_at:\n",
    "                    days_in_step = calculate_days_in_step(moved_to_step_at)\n",
    "                else:\n",
    "                    days_in_step = \"Unknown\"\n",
    "\n",
    "                # If removed, show the removed date\n",
    "                if removed_at:\n",
    "                    removed_at_datetime = datetime.strptime(removed_at, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=timezone.utc)\n",
    "                    removed = True\n",
    "                    date_removed = removed_at_datetime.strftime('%Y-%m-%d')\n",
    "                else:\n",
    "                    removed = False\n",
    "                    date_removed = \"N/A\"\n",
    "\n",
    "                # Fetch step history for the person\n",
    "                step_history = get_step_history_for_csv(workflow_id, person_id)\n",
    "\n",
    "                # Append data for CSV\n",
    "                data_rows.append({\n",
    "                    'Campus Name': campus_name,\n",
    "                    'Workflow Name': workflow_name,\n",
    "                    'Workflow Step': workflow_step,\n",
    "                    'Workflow Step ID': current_step_id,\n",
    "                    'Person Name': person_name,\n",
    "                    'Assignee Name': assignee_name,\n",
    "                    'Assignee Email': assignee_email,\n",
    "                    'Moved to Step': moved_to_step_at if moved_to_step_at else \"Unknown\",\n",
    "                    'Completed_at': completed_at,\n",
    "                    'Removed': removed,\n",
    "                    'Date Removed': date_removed,\n",
    "                    'Days in Step': days_in_step,\n",
    "                    'Step History': step_history\n",
    "                })\n",
    "\n",
    "            # Write data to CSV\n",
    "            csv_columns = ['Campus Name', 'Workflow Name', 'Workflow Step','Workflow Step ID', 'Person Name', 'Assignee Name', 'Assignee Email', 'Moved to Step', 'Completed_at','Removed', 'Date Removed', 'Days in Step', 'Step History']\n",
    "            try:\n",
    "                with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(data_rows)\n",
    "                print(f\"CSV file '{csv_filename}' created successfully.\")\n",
    "            except IOError:\n",
    "                print(\"I/O error while writing the CSV file.\")\n",
    "        else:\n",
    "            print(\"No people found in the workflow.\")\n",
    "    else:\n",
    "        print(\"No steps found for this workflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VRKKzDHWA83l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'workflow_data_548715.csv' created successfully.\n",
      "CSV file 'workflow_data_544795.csv' created successfully.\n",
      "CSV file 'workflow_data_544778.csv' created successfully.\n",
      "CSV file 'workflow_data_564585.csv' created successfully.\n",
      "CSV file 'workflow_data_544755.csv' created successfully.\n",
      "Error fetching activities: 429, {'errors': [{'code': '429', 'title': 'Too May Requests', 'detail': 'Rate limit exceeded: 101 of 100 requests per 20 seconds'}]}\n",
      "Rate limit exceeded. Waiting 1 seconds before retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m workflow_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m548715\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544795\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544778\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m564585\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544755\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544593\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550431\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550408\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544671\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544864\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544777\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544842\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550389\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m541922\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550371\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544885\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544809\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m564649\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m548180\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544664\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550421\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544725\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550386\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544846\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m555089\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m564646\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m548148\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544649\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550418\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544718\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550368\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544881\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m544844\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m550397\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m561167\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m workflow_id \u001b[38;5;129;01min\u001b[39;00m workflow_ids:\n\u001b[0;32m----> 5\u001b[0m   list_people_by_workflow_step_to_csv(workflow_id, csv_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkflow_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkflow_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 322\u001b[0m, in \u001b[0;36mlist_people_by_workflow_step_to_csv\u001b[0;34m(workflow_id, csv_filename)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Get assignee name and email\u001b[39;00m\n\u001b[1;32m    321\u001b[0m assignee_name \u001b[38;5;241m=\u001b[39m get_person_name(assignee_id) \u001b[38;5;28;01mif\u001b[39;00m assignee_id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown Assignee\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 322\u001b[0m assignee_email \u001b[38;5;241m=\u001b[39m get_person_email(assignee_id) \u001b[38;5;28;01mif\u001b[39;00m assignee_id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown Email\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Get the date they were moved to the current step\u001b[39;00m\n\u001b[1;32m    325\u001b[0m moved_to_step_at \u001b[38;5;241m=\u001b[39m person_card[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoved_to_step_at\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m, in \u001b[0;36mget_person_email\u001b[0;34m(person_id, retries, batch_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPEOPLE_BASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mperson_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/emails?offset=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&per_page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 71\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, auth\u001b[38;5;241m=\u001b[39mHTTPBasicAuth(API_APP_ID, API_SECRET))\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     74\u001b[0m         emails_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:652\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[1;32m    650\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 652\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[1;32m    653\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    654\u001b[0m     cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[1;32m    655\u001b[0m     ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[1;32m    656\u001b[0m     ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[1;32m    657\u001b[0m     ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[1;32m    658\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[1;32m    659\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[1;32m    660\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[1;32m    661\u001b[0m     cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[1;32m    662\u001b[0m     key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[1;32m    663\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[1;32m    664\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname_rm_dot,\n\u001b[1;32m    665\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[1;32m    666\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    667\u001b[0m     assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[1;32m    668\u001b[0m     assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[1;32m    669\u001b[0m )\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# Forwarding proxies can never have a verified target since\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# the proxy is the one doing the verification. Should instead\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# use a CONNECT tunnel in order to verify the target.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# See: https://github.com/urllib3/urllib3/issues/3267.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:805\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    803\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 805\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    806\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    807\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[1;32m    808\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[1;32m    809\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[1;32m    810\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[1;32m    811\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[1;32m    812\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[1;32m    813\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    814\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    815\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    816\u001b[0m )\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    506\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[1;32m    456\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    457\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[1;32m    458\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[1;32m    459\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[1;32m    460\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    461\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[1;32m    463\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "workflow_ids = ['548715', '544795', '544778', '564585', '544755', '544593', '550431', '550408', '544671', '544864', '544777', '544842', '550389', '541922', '550371', '544885', '544809', '564649', '548180', '544664', '550421', '544725', '550386', '544846', '555089', '564646', '548148', '544649', '550418', '544718', '550368', '544881', '544844', '550397', '561167']\n",
    "\n",
    "\n",
    "for workflow_id in workflow_ids:\n",
    "  list_people_by_workflow_step_to_csv(workflow_id, csv_filename=f'workflow_data_{workflow_id}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej5AStyo7NOL"
   },
   "source": [
    "## Combine and Clean up files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtowhVw3v20A"
   },
   "outputs": [],
   "source": [
    "def combine_csv_files(output_filename='combined_workflow_data.csv'):\n",
    "    # Use glob to find all CSV files matching the pattern\n",
    "    csv_files = glob.glob('workflow_data_*.csv')\n",
    "\n",
    "    # List to hold all DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through all matching CSV files\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Read each CSV file and append to the list of dataframes\n",
    "            df = pd.read_csv(csv_file)\n",
    "            dataframes.append(df)\n",
    "            print(f\"Loaded {csv_file} successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {csv_file}: {e}\")\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        # Save the combined dataframe to a new CSV file\n",
    "        combined_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Combined CSV saved to {output_filename}\")\n",
    "    else:\n",
    "        print(\"No CSV files found or loaded.\")\n",
    "\n",
    "# Call the function to combine CSV files\n",
    "combine_csv_files('combined_workflow_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FScTBONBnXtU"
   },
   "outputs": [],
   "source": [
    "def fill_missing_workflow_data(df, step_column='Workflow Step', id_column='Workflow Step ID'):\n",
    "    \"\"\"\n",
    "    Function to fill missing Workflow Step or Workflow Step ID based on existing data in other rows.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame containing the workflow data.\n",
    "    step_column (str): The name of the Workflow Step column.\n",
    "    id_column (str): The name of the Workflow Step ID column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with missing Workflow Step or Workflow Step ID filled in.\n",
    "    \"\"\"\n",
    "    # Step 1: Identify the complete pairs (rows where both columns are present)\n",
    "    complete_pairs = df.dropna(subset=[step_column, id_column]).drop_duplicates(subset=[step_column, id_column])\n",
    "\n",
    "    # Create a dictionary mapping Workflow Step to Workflow Step ID and vice versa\n",
    "    step_to_id = dict(zip(complete_pairs[step_column], complete_pairs[id_column]))\n",
    "    id_to_step = dict(zip(complete_pairs[id_column], complete_pairs[step_column]))\n",
    "\n",
    "    # Step 2: Fill missing Workflow Step using Workflow Step ID\n",
    "    df[step_column] = df.apply(\n",
    "        lambda row: id_to_step.get(row[id_column], row[step_column]) if pd.isna(row[step_column]) else row[step_column],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Step 3: Fill missing Workflow Step ID using Workflow Step\n",
    "    df[id_column] = df.apply(\n",
    "        lambda row: step_to_id.get(row[step_column], row[id_column]) if pd.isna(row[id_column]) else row[id_column],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df = pd.read_csv('combined_workflow_data.csv')\n",
    "\n",
    "# Call the function to fill in missing data\n",
    "filled_df = fill_missing_workflow_data(df)\n",
    "\n",
    "# Display the result\n",
    "filled_df.head()\n",
    "filled_df.to_csv('filled_workflow_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
